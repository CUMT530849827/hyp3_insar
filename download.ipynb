{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import zipfile\n",
    "from zipfile import BadZipFile\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import hyp3_sdk as sdk\n",
    "from hyp3_sdk import asf_search\n",
    "from tqdm.notebook import trange, tqdm\n",
    "# from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter Hyp3 credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp3 = sdk.HyP3(prompt=True)\n",
    "my_info = hyp3.my_info()\n",
    "print(f\"Remaining Quota: {my_info['quota']['remaining']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create download list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project name\n",
    "# project_name = 'ifg_a_p81_f1048_20x4'\n",
    "project_name = 'p81f1048_10x2_2'\n",
    "\n",
    "# Set date bounds\n",
    "start_date = datetime.datetime(2017, 12, 31)\n",
    "end_date = datetime.datetime(2021, 12, 31)\n",
    "\n",
    "# Read in \n",
    "df = pd.read_csv('slcs_asc_path81_frame1048.csv',usecols=['Granule Name', 'Acquisition Date'])\n",
    "\n",
    "# Convert and limit by date\n",
    "df['Acquisition Date'] = pd.to_datetime(df['Acquisition Date'], format='%Y-%m-%dT%H:%M:%S')\n",
    "df = df.loc[(df['Acquisition Date']>=start_date) & (df['Acquisition Date']<=end_date)]\n",
    "df = df.sort_values('Acquisition Date')\n",
    "\n",
    "granules = list(df['Granule Name'])\n",
    "print(f'Will create interferograms for {len(granules)} SLCs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Request Ifg generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insar_jobs = sdk.Batch()\n",
    "first = True\n",
    "for reference in tqdm(granules):\n",
    "    neighbors_metadata = asf_search.get_nearest_neighbors(reference, max_neighbors=3)\n",
    "    for secondary_metadata in neighbors_metadata:\n",
    "        secondary = secondary_metadata['granuleName']\n",
    "        if first:\n",
    "            insar_jobs += hyp3.submit_insar_job(reference, secondary, name=project_name, include_inc_map=True,\n",
    "                                                include_dem=True, looks='10x2', include_wrapped_phase=False)\n",
    "            first = False\n",
    "        else:\n",
    "            insar_jobs += hyp3.submit_insar_job(reference, secondary, name=project_name, looks='10x2', include_wrapped_phase=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check progress and download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch = hyp3.find_jobs(name=project_name)\n",
    "\n",
    "if batch.complete():\n",
    "    #filter to only succeeded jobs\n",
    "    succeeded_jobs = batch.filter_jobs(succeeded=True, running=False, failed=False)\n",
    "\n",
    "    #download files if not downloaded already\n",
    "    for job in succeeded_jobs.jobs:\n",
    "        filename = job.to_dict()['files'][0]['filename']\n",
    "        location = os.path.join(project_name,filename)\n",
    "        if not os.path.exists(location):\n",
    "            job.download_files(location=project_name,create=True)\n",
    "        \n",
    "    # #download files if not downloaded already\n",
    "    # file_list = succeeded_jobs.download_files(location=project_name,create=True)\n",
    "else:\n",
    "    #to get updated information\n",
    "    batch = hyp3.refresh(batch)\n",
    "    #or to wait until completion and get updated information (which will take a fair bit)\n",
    "    batch = hyp3.watch(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzip files and clip to same extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/mnt/i/hyp3_downloads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter to only succeeded jobs\n",
    "succeeded_jobs = batch.filter_jobs(succeeded=True, running=False, failed=False)\n",
    "\n",
    "#download files if not downloaded already\n",
    "for job in succeeded_jobs.jobs:\n",
    "    filename = job.to_dict()['files'][0]['filename']\n",
    "    location = os.path.join(project_name,filename)\n",
    "    if not os.path.exists(location):\n",
    "        job.download_files(location=project_name,create=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project_name = 'tongariro_asc'\n",
    "\n",
    "zips = glob.glob(os.path.join(project_name,'*.zip'))\n",
    "unws = [[x,f'{os.path.join(x.split(\".\")[0],os.path.basename(x).split(\".\")[0])}_unw_phase.tif'] for x in zips]\n",
    "\n",
    "zips = [x[0] for x in unws if not os.path.exists(x[1])]\n",
    "folders = [x.split('.')[0] for x in zips]\n",
    "\n",
    "bad_files=[]\n",
    "\n",
    "for zip_file, folder in tqdm(zip(zips, folders), total=len(zips)):\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "            zip_ref.extractall(project_name)\n",
    "    except BadZipFile:\n",
    "        bad_files.append(os.path.basename(zip_file))\n",
    "\n",
    "if len(bad_files) > 0:\n",
    "    print(f'These files were invalid zips:\\n{bad_files}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folders = glob.glob(os.path.join(project_name,'S1*'))\n",
    "# all_images = []\n",
    "# for suffix in ['dem','inc_map','water_map','unw_phase','corr']:\n",
    "#     all_images += [os.path.join(x,f'{os.path.basename(x)}_{suffix}.tif') for x in folders]\n",
    "\n",
    "# exists = [x for x in all_images if os.path.exists(x)]\n",
    "# to_clip = [x for x in exists if not os.path.exists(f'{x[:-4]}_clip.tif')]\n",
    "\n",
    "# cutGeotiffs.cutFiles(to_clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done!"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d0805dd0a63a9aa5b56409bdcec0789bd27e38c20d1aed221282d406ad418a8c"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "d0805dd0a63a9aa5b56409bdcec0789bd27e38c20d1aed221282d406ad418a8c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
