{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import zipfile\n",
    "from zipfile import BadZipFile\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import hyp3_sdk as sdk\n",
    "from hyp3_sdk import asf_search\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from hyp3lib import cutGeotiffs\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "# from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'desc_lower_211106'\n",
    "out_dir = Path('../desc_lower').resolve()\n",
    "\n",
    "if not out_dir.is_dir():\n",
    "    out_dir.mkdir()\n",
    "else:\n",
    "    print('directory already exists!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdf = gpd.read_file('desc-lower-2021-10-21.geojson')\n",
    "asf = Path('asf_files')\n",
    "gdf = gpd.read_file(asf/'desc-lower-2021-11-05.geojson')\n",
    "min_date = pd.to_datetime('20140201T000000')\n",
    "max_date = pd.to_datetime('20211230T000000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gdf.loc[:,['fileName','startTime','geometry']]\n",
    "gdf = gdf.to_crs(epsg=32760)\n",
    "gdf['startTime'] = pd.to_datetime(gdf.startTime)\n",
    "gdf = gdf.sort_values('startTime').reset_index(drop=True)\n",
    "# gdf = gdf.loc[gdf.startTime <= max_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = gdf.loc[gdf.startTime==gdf.startTime.max()]\n",
    "union = gpd.overlay(gdf, template, how='intersection')\n",
    "gdf['intersect'] = union.area / template.area.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(gdf.startTime,gdf.intersect)\n",
    "print(f'n ifgs: {len(gdf)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gap = gdf.loc[(gdf.startTime>=pd.to_datetime('20160415')) & (gdf.startTime<=pd.to_datetime('20160530'))].copy()\n",
    "gap['startTime'] = gap['startTime'].dt.strftime('%y%m%d')\n",
    "# gap.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_date = pd.to_datetime('20180101T000000')\n",
    "min_overlap = 0.99\n",
    "slcs = gdf[(gdf.intersect>min_overlap) & (gdf.startTime>=min_date)].copy().reset_index(drop=True)\n",
    "print(f'Removed {gdf.shape[0] - slcs.shape[0]} of {gdf.shape[0]} SLCs')\n",
    "\n",
    "plt.scatter(slcs.startTime,slcs.intersect)\n",
    "y = plt.ylim((min_overlap,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = {}\n",
    "todo = 0\n",
    "names = slcs.fileName.str[:-4]\n",
    "length = names.shape[0]\n",
    "\n",
    "completed = ['_'.join(x.name.split('_')[1:3]) for x in out_dir.glob('*zip')]\n",
    "n_complete = len(completed)\n",
    "\n",
    "for i,reference_name in enumerate(names):\n",
    "    reference_date = reference_name.split('_')[5]\n",
    "    secondary = []\n",
    "    \n",
    "    for j in [1,2,3]:\n",
    "        index = i+j\n",
    "        secondary_name = None\n",
    "        date12 = None\n",
    "        \n",
    "        if index < length:\n",
    "            secondary_name = names[index]\n",
    "            secondary_date = secondary_name.split('_')[5]\n",
    "            date12 = f'{reference_date}_{secondary_date}'\n",
    "        \n",
    "        if (secondary_name != None) & (date12 not in completed):\n",
    "            secondary.append(names[index])\n",
    "            todo += 1\n",
    "        elif (secondary_name != None) & (date12 in completed):\n",
    "            completed.remove(date12)\n",
    "\n",
    "    if secondary != []:\n",
    "        pairs[reference_name] = secondary\n",
    "\n",
    "print(f'{todo} interferograms need to be requested, {len(completed)} are done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter Hyp3 credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp3 = sdk.HyP3(prompt=True)\n",
    "\n",
    "my_info = hyp3.my_info()\n",
    "print(f\"Remaining Quota: {my_info['quota']['remaining']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Request Ifg generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insar_jobs = sdk.Batch()\n",
    "first = True\n",
    "\n",
    "options = {'name':project_name,'looks':'10x2', 'include_wrapped_phase':False, 'apply_water_mask':True}\n",
    "initial_options = dict(options)\n",
    "initial_options.update({'include_inc_map':True,'include_dem':True,'include_look_vectors':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for reference in tqdm(pairs):\n",
    "    for secondary in pairs[reference]:\n",
    "        if first:\n",
    "            insar_jobs += hyp3.submit_insar_job(reference, secondary, **initial_options)\n",
    "            first = False\n",
    "        else:\n",
    "            insar_jobs += hyp3.submit_insar_job(reference, secondary, **options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check progress and download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start = datetime.datetime.today() - datetime.timedelta(days=3)\n",
    "\n",
    "batch = hyp3.find_jobs(name=project_name,start=start)\n",
    "\n",
    "if batch.complete():\n",
    "    #filter to only succeeded jobs\n",
    "    succeeded_jobs = batch.filter_jobs(succeeded=True, running=False, failed=False)\n",
    "    \n",
    "    complete = 0\n",
    "    #download files if not downloaded already\n",
    "    for job in succeeded_jobs.jobs:\n",
    "        filename = job.to_dict()['files'][0]['filename']\n",
    "        # location = os.path.join(project_name,filename)\n",
    "        # if not os.path.exists(location):\n",
    "        #     job.download_files(location=project_name,create=True)\n",
    "        location = out_dir / filename\n",
    "        if not location.exists():\n",
    "            job.download_files(location=str(out_dir),create=True)\n",
    "        else:\n",
    "            complete += 1\n",
    "\n",
    "    print(f'{complete} files already downloaded!')\n",
    "\n",
    "else:\n",
    "    #to get updated information\n",
    "    batch = hyp3.refresh(batch)\n",
    "    #or to wait until completion and get updated information (which will take a fair bit)\n",
    "    batch = hyp3.watch(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzip files and clip to same extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zips = list(out_dir.glob('*.zip'))\n",
    "unws = [x.with_suffix('') / f'{x.name.split(\".\")[0]}_unw_phase.tif' for x in zips]\n",
    "\n",
    "zips = [x for x,y in zip(zips,unws) if not y.exists()]\n",
    "print(f'{len(zips)} files to unzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_files=[]\n",
    "\n",
    "for zip_file in tqdm(zips, total=len(zips)):\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "            zip_ref.extractall(out_dir)\n",
    "    except BadZipFile:\n",
    "        bad_files.append(os.path.basename(zip_file))\n",
    "\n",
    "if len(bad_files) > 0:\n",
    "    print(f'These files were invalid zips:\\n{bad_files}')\n",
    "else:\n",
    "    print('All files were unzipped successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [x for x in out_dir.glob('S1*') if x.is_dir()]\n",
    "all_images = []\n",
    "\n",
    "for suffix in ['dem','lv_theta','water_mask','unw_phase','corr']:\n",
    "    images = [x / f'{x.name}_{suffix}.tif' for x in folders]\n",
    "    images = [x for x in images if x.exists()]\n",
    "    \n",
    "    # We only need the lv_theta and water_mask files found in the dem directory\n",
    "    if suffix == 'dem':\n",
    "        prefix = str(images[0]).replace('dem.tif','')\n",
    "    elif suffix not in ['unw_phase','corr']:\n",
    "        images = [x for x in images if prefix in str(x)]\n",
    "    \n",
    "    print(f'including {len(images)} {suffix} images')\n",
    "    all_images += images\n",
    "\n",
    "to_clip = [str(x) for x in all_images if not Path(str(x).replace('.tif','_clip.tif')).exists()]\n",
    "print(f'{len(to_clip)} images to clip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cutGeotiffs.cutFiles(to_clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done!"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d0805dd0a63a9aa5b56409bdcec0789bd27e38c20d1aed221282d406ad418a8c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "metadata": {
   "interpreter": {
    "hash": "d0805dd0a63a9aa5b56409bdcec0789bd27e38c20d1aed221282d406ad418a8c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
